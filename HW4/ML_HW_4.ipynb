{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML HW2 Xinmeng Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# scientific computing libraries\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import scipy.stats as st\n",
    "from collections import Counter\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and labels and normalize it\n",
    "mnist = np.load('MNIST_data.npy') / 255.0\n",
    "mnist_labels = np.load('MNIST_labels.npy')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have your data stored in numpy arrays X and Y\n",
    "# Split the data into 80% training and 20% test\n",
    "mnist_train, mnist_test, train_labels, test_labels= train_test_split(mnist, mnist_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into 60% training and 20% development\n",
    "mnist_train, mnist_dev, train_labels, dev_labels = train_test_split(mnist_train, train_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# mnist_train, train_labels: Training data (60% of the original data)\n",
    "# mnist_dev, dev_labels: Development data (20% of the original data)\n",
    "# mnist_test, test_labels: Test data (20% of the original data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: x-large;\">Bayes rule with Bernoulli mixtures</span>\n",
    "\n",
    "Read in the MNIST data set and binarize as in HW3. Estimate a separate mixture\n",
    "model for each class with $M = 1,5,10,20$. So in total you will have $M \\cdot 10$ models.\n",
    "For M = 1 you don’t need the EM algorithm. Use the development set to choose\n",
    "the optimal M and then retrain the mixtures with the training and development sets.\n",
    "Classify the test set data using Bayes’ rule with your estimated class mixture models.\n",
    "You can assume that $\\pi_c = 1/10, c = 1,...,10.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the data\n",
    "train_X_binarized = (mnist_train > 0.5).astype(int)\n",
    "dev_X_binarized = (mnist_dev > 0.5).astype(int)\n",
    "test_X_binarized = (mnist_test > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_algorithm_binarized(X, num_components, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Runs the EM algorithm on binarized data X to fit a mixture of Bernoulli distributions.\n",
    "    Returns the learned parameters pi, mu, and w.\n",
    "    \"\"\"\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize pi and w matrix randomly\n",
    "    pi = np.random.rand(num_components)\n",
    "    pi /= pi.sum()\n",
    "    w = np.random.rand(num_components, num_features)\n",
    "    w /= w.sum\n",
    "\n",
    "    # Run EM algorithm\n",
    "    for i in range(num_iterations):\n",
    "        # E-step: compute responsibilities\n",
    "        z = X.dot(w.T)\n",
    "        z *= pi\n",
    "        \n",
    "        # Normalize responsibilities\n",
    "        z /= z.sum\n",
    "        \n",
    "        # M-step: update parameters\n",
    "        w = z.T.dot(X) / z.sum\n",
    "\n",
    "        mu = (w + 1) / 2  # Avoid probabilities of 0 or 1\n",
    "        pi = z.mean(axis=0)\n",
    "        \n",
    "    # Return learned parameters\n",
    "    return pi, mu, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Train mixture models for each class with M components\n",
    "M_values = [1, 5, 10, 20]  # M values to use\n",
    "pi_dict = {}  # Dictionary to store learned pi values for each class and M\n",
    "mu_dict = {}  # Dictionary to store learned mu values for each class and M\n",
    "\n",
    "for M in M_values:\n",
    "    for c in range(10):\n",
    "        if M == 1:\n",
    "            # For M=1, use a uniform distribution as the mixture model\n",
    "            pi = np.ones(1)\n",
    "            mu = np.random.rand(1)\n",
    "        else:\n",
    "            # Extract data for current class\n",
    "            class_indices = np.where(train_labels == str(c))[0]\n",
    "            if len(class_indices) == 0:\n",
    "                # Skip if there are no samples for current class\n",
    "                continue\n",
    "            class_X = train_X_binarized[class_indices]\n",
    "            \n",
    "            # Train mixture model for current class with M components\n",
    "            pi, mu, w = EM_algorithm_binarized(class_X, num_components=M)\n",
    "        \n",
    "        # Store learned parameters for current class and M\n",
    "        pi_dict[(c, M)] = pi\n",
    "        mu_dict[(c, M)] = mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.])])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mu_dict.keys()\n",
    "pi_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store mixture models for different M values for each class\n",
    "mixture_models = {}\n",
    "\n",
    "# Loop over each class\n",
    "for c in range(10):\n",
    "    print(\"Fitting class\", c)\n",
    "    X_train_c = np.where(train_labels == str(c))\n",
    "    X_dev_c = np.where(dev_labels == str(c))\n",
    "    \n",
    "    # Initialize dictionary to store mixture models for this class\n",
    "    mixture_models_c = {}\n",
    "    \n",
    "    # Loop over each M value\n",
    "    for M in M_values:\n",
    "        if M == 1:\n",
    "            # If M is 1, use a single Bernoulli distribution (no need for EM)\n",
    "            pi, mu, w = EM_algorithm_binarized(X_train_c, 1, num_iterations=0)\n",
    "        else:\n",
    "            # Otherwise, use EM to estimate mixture of Bernoulli distributions\n",
    "            pi, mu, w = EM_algorithm_binarized(X_train_c, M)\n",
    "        \n",
    "        # Store the trained mixture model for this M value\n",
    "        mixture_models_c[M] = (pi, mu, w)\n",
    "    \n",
    "    # Choose the optimal M value based on the development set\n",
    "    best_M = None\n",
    "    best_score = None\n",
    "    for M in M_values:\n",
    "        pi, mu, w = mixture_models_c[M]\n",
    "        dev_score = np.sum(X_dev_c.dot(np.log(mu).T) + (1 - X_dev_c).dot(np.log(1 - mu).T) + np.log(pi), axis=1)\n",
    "        dev_score = dev_score.mean()\n",
    "        if best_score is None or dev_score > best_score:\n",
    "            best_M = M\n",
    "            best_score = dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi,mu,w = EM_algorithm_binarized(mnist_test, num_components=5)\n",
    "num_test_samples = mnist_test.shape[0]\n",
    "log_probs = np.dot(mnist_test, np.log(w.T)) + np.dot(1 - mnist_test, np.log(1 - w.T))\n",
    "log_probs += np.log(pi)\n",
    "predicted_labels = np.argmax(log_probs, axis=1) + 1  # Add 1 to convert from zero-indexed to one-indexed labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: x-large;\">Logistic Regression</span>\n",
    "1. Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on development set: 0.9185\n",
      "C = 1e-05 Accuracy on development set: 0.7122142857142857\n",
      "C = 0.0001 Accuracy on development set: 0.8388571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.001 Accuracy on development set: 0.8872857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01 Accuracy on development set: 0.9103571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1 Accuracy on development set: 0.9235714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1.0 Accuracy on development set: 0.9212857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 10.0 Accuracy on development set: 0.9193571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 100.0 Accuracy on development set: 0.9184285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1000.0 Accuracy on development set: 0.9188571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 10000.0 Accuracy on development set: 0.9187857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 100000.0 Accuracy on development set: 0.9185\n",
      "Accuracy on test set: 0.9217857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lg=LogisticRegression(fit_intercept=True, C=100000, penalty='l2', multi_class='multinomial',solver='lbfgs')\n",
    "\n",
    "# Train the model on the training set\n",
    "lg.fit(mnist_train, train_labels)\n",
    "\n",
    "# Evaluate the model on the development set\n",
    "dev_predictions = lg.predict(mnist_dev)\n",
    "dev_accuracy = accuracy_score(dev_labels, dev_predictions)\n",
    "print(\"Accuracy on development set:\", dev_accuracy)\n",
    "\n",
    "# Search for the optimal value of C on the development set\n",
    "best_c = None\n",
    "best_accuracy = 0.0\n",
    "c_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 100000.0]\n",
    "for c in c_values:\n",
    "    lg = LogisticRegression(fit_intercept=True, C=c, penalty='l2', multi_class='multinomial', solver='lbfgs')\n",
    "    lg.fit(mnist_train, train_labels)\n",
    "    dev_predictions = lg.predict(mnist_dev)\n",
    "    dev_accuracy = accuracy_score(dev_labels, dev_predictions)\n",
    "    print(\"C =\", c, \"Accuracy on development set:\", dev_accuracy)\n",
    "    if dev_accuracy > best_accuracy:\n",
    "        best_accuracy = dev_accuracy\n",
    "        best_c = c\n",
    "\n",
    "# Retrain the model with the optimal value of C using training and development set combined\n",
    "lg = LogisticRegression(fit_intercept=True, C=best_c, penalty='l2', multi_class='multinomial', solver='lbfgs')\n",
    "lg.fit(np.concatenate((mnist_train, mnist_dev)), np.concatenate((train_labels, dev_labels)))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = lg.predict(mnist_test)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Accuracy on test set:\", test_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA function\n",
    "def PCA(X, k):\n",
    "    # center data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # compute covariance matrix\n",
    "    cov_matrix = np.cov(X_centered.T)\n",
    "    \n",
    "    # compute eigenvectors and eigenvalues\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    \n",
    "    # sort eigenvectors by eigenvalues\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvectors = eigenvectors[:,idx]\n",
    "    \n",
    "    # return first k eigenvectors\n",
    "    return eigenvectors[:,:k], eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 C = 1e-05 Accuracy on development set: 0.21614285714285714\n",
      "k = 1 C = 0.0001 Accuracy on development set: 0.27214285714285713\n",
      "k = 1 C = 0.001 Accuracy on development set: 0.3048571428571429\n",
      "k = 1 C = 0.01 Accuracy on development set: 0.3105\n",
      "k = 1 C = 0.1 Accuracy on development set: 0.3107857142857143\n",
      "k = 1 C = 1.0 Accuracy on development set: 0.3107142857142857\n",
      "k = 1 C = 10.0 Accuracy on development set: 0.3107857142857143\n",
      "k = 1 C = 100.0 Accuracy on development set: 0.3107857142857143\n",
      "k = 1 C = 1000.0 Accuracy on development set: 0.3107857142857143\n",
      "k = 1 C = 10000.0 Accuracy on development set: 0.3107857142857143\n",
      "k = 1 C = 100000.0 Accuracy on development set: 0.3107857142857143\n",
      "k = 5 C = 1e-05 Accuracy on development set: 0.5146428571428572\n",
      "k = 5 C = 0.0001 Accuracy on development set: 0.6122142857142857\n",
      "k = 5 C = 0.001 Accuracy on development set: 0.6565714285714286\n",
      "k = 5 C = 0.01 Accuracy on development set: 0.6680714285714285\n",
      "k = 5 C = 0.1 Accuracy on development set: 0.6699285714285714\n",
      "k = 5 C = 1.0 Accuracy on development set: 0.6702142857142858\n",
      "k = 5 C = 10.0 Accuracy on development set: 0.6701428571428572\n",
      "k = 5 C = 100.0 Accuracy on development set: 0.6701428571428572\n",
      "k = 5 C = 1000.0 Accuracy on development set: 0.6701428571428572\n",
      "k = 5 C = 10000.0 Accuracy on development set: 0.6701428571428572\n",
      "k = 5 C = 100000.0 Accuracy on development set: 0.6701428571428572\n",
      "k = 10 C = 1e-05 Accuracy on development set: 0.6427142857142857\n",
      "k = 10 C = 0.0001 Accuracy on development set: 0.7496428571428572\n",
      "k = 10 C = 0.001 Accuracy on development set: 0.7901428571428571\n",
      "k = 10 C = 0.01 Accuracy on development set: 0.8021428571428572\n",
      "k = 10 C = 0.1 Accuracy on development set: 0.8041428571428572\n",
      "k = 10 C = 1.0 Accuracy on development set: 0.8047142857142857\n",
      "k = 10 C = 10.0 Accuracy on development set: 0.8047857142857143\n",
      "k = 10 C = 100.0 Accuracy on development set: 0.8048571428571428\n",
      "k = 10 C = 1000.0 Accuracy on development set: 0.8048571428571428\n",
      "k = 10 C = 10000.0 Accuracy on development set: 0.8048571428571428\n",
      "k = 10 C = 100000.0 Accuracy on development set: 0.8048571428571428\n",
      "k = 50 C = 1e-05 Accuracy on development set: 0.7099285714285715\n",
      "k = 50 C = 0.0001 Accuracy on development set: 0.8355714285714285\n",
      "k = 50 C = 0.001 Accuracy on development set: 0.8802142857142857\n",
      "k = 50 C = 0.01 Accuracy on development set: 0.9000714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50 C = 0.1 Accuracy on development set: 0.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50 C = 1.0 Accuracy on development set: 0.9057142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50 C = 10.0 Accuracy on development set: 0.9058571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50 C = 100.0 Accuracy on development set: 0.9057857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50 C = 1000.0 Accuracy on development set: 0.9057857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50 C = 10000.0 Accuracy on development set: 0.9057142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50 C = 100000.0 Accuracy on development set: 0.9057142857142857\n",
      "k = 100 C = 1e-05 Accuracy on development set: 0.7099285714285715\n",
      "k = 100 C = 0.0001 Accuracy on development set: 0.8355714285714285\n",
      "k = 100 C = 0.001 Accuracy on development set: 0.8802142857142857\n",
      "k = 100 C = 0.01 Accuracy on development set: 0.9000714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100 C = 0.1 Accuracy on development set: 0.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100 C = 1.0 Accuracy on development set: 0.9057142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100 C = 10.0 Accuracy on development set: 0.9058571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100 C = 100.0 Accuracy on development set: 0.9057857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100 C = 1000.0 Accuracy on development set: 0.9057857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100 C = 10000.0 Accuracy on development set: 0.9057142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100 C = 100000.0 Accuracy on development set: 0.9057142857142857\n",
      "k = 200 C = 1e-05 Accuracy on development set: 0.7099285714285715\n",
      "k = 200 C = 0.0001 Accuracy on development set: 0.8355714285714285\n",
      "k = 200 C = 0.001 Accuracy on development set: 0.8802142857142857\n",
      "k = 200 C = 0.01 Accuracy on development set: 0.9000714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 200 C = 0.1 Accuracy on development set: 0.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 200 C = 1.0 Accuracy on development set: 0.9057142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 200 C = 10.0 Accuracy on development set: 0.9058571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 200 C = 100.0 Accuracy on development set: 0.9057857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 200 C = 1000.0 Accuracy on development set: 0.9057857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 200 C = 10000.0 Accuracy on development set: 0.9057142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 200 C = 100000.0 Accuracy on development set: 0.9057142857142857\n",
      "Optimal k: 50\n",
      "Optimal C: 10.0\n",
      "Accuracy on test set: 0.9062857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\cse160\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Perform PCA on the training set to obtain the principal components\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(mnist_train)\n",
    "train_pcs = pca.transform(mnist_train)\n",
    "\n",
    "# Perform PCA on the development set to obtain the principal components\n",
    "dev_pcs = pca.transform(mnist_dev)\n",
    "\n",
    "# Perform PCA on the test set to obtain the principal components\n",
    "test_pcs = pca.transform(mnist_test)\n",
    "\n",
    "# Search for the optimal number of principal components (k) and optimal value of C on the development set\n",
    "best_k = None\n",
    "best_c = None\n",
    "best_accuracy = 0.0\n",
    "k_values = [1, 5, 10, 50, 100, 200]\n",
    "c_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 100000.0]\n",
    "for k in k_values:\n",
    "    # Train logistic regression model with k principal components for each value of C\n",
    "    for c in c_values:\n",
    "        lg = LogisticRegression(fit_intercept=True, C=c, penalty='l2', multi_class='multinomial', solver='lbfgs')\n",
    "        lg.fit(train_pcs[:, :k], train_labels)\n",
    "        dev_predictions = lg.predict(dev_pcs[:, :k])\n",
    "        dev_accuracy = accuracy_score(dev_labels, dev_predictions)\n",
    "        print(\"k =\", k, \"C =\", c, \"Accuracy on development set:\", dev_accuracy)\n",
    "        if dev_accuracy > best_accuracy:\n",
    "            best_accuracy = dev_accuracy\n",
    "            best_k = k\n",
    "            best_c = c\n",
    "\n",
    "# Retrain the model with the optimal number of principal components and optimal value of C\n",
    "lg = LogisticRegression(fit_intercept=True, C=best_c, penalty='l2', multi_class='multinomial', solver='lbfgs')\n",
    "lg.fit(np.concatenate((train_pcs[:, :best_k], dev_pcs[:, :best_k])), np.concatenate((train_labels, dev_labels)))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = lg.predict(test_pcs[:, :best_k])\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Optimal k:\", best_k)\n",
    "print(\"Optimal C:\", best_c)\n",
    "print(\"Accuracy on test set:\", test_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: x-large;\">Stochastic gradient descent</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax, logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sgd(X, y, X_val, y_val, reg_param, learning_rate, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Implements stochastic gradient descent (SGD) for logistic regression with L2 regularization.\n",
    "\n",
    "    Args:\n",
    "    X: array, shape (num_train, num_features), training data.\n",
    "    y: array, shape (num_train,), training labels.\n",
    "    X_val: array, shape (num_val, num_features), validation data.\n",
    "    y_val: array, shape (num_val,), validation labels.\n",
    "    reg_param: float, regularization parameter.\n",
    "    learning_rate: float, learning rate.\n",
    "    batch_size: int, mini-batch size.\n",
    "    num_epochs: int, number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "    W: array, shape (num_features, num_classes), learned weights.\n",
    "    b: array, shape (num_classes,), learned biases.\n",
    "    train_losses: list, training losses for each epoch.\n",
    "    val_losses: list, validation losses for each epoch.\n",
    "    train_error_rates: list, training error rates for each epoch.\n",
    "    val_error_rates: list, validation error rates for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_train, num_features = X.shape\n",
    "    num_val = X_val.shape[0]\n",
    "    num_classes = np.max(y) + 1\n",
    "\n",
    "    # Initialize model parameters\n",
    "    W = np.zeros((num_features, num_classes))\n",
    "    b = np.zeros(num_classes)\n",
    "\n",
    "    # Initialize lists to store losses and error rates for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_error_rates = []\n",
    "    val_error_rates = []\n",
    "\n",
    "    # Perform SGD\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the training data for each epoch\n",
    "        permutation = np.random.permutation(num_train)\n",
    "        X = X[permutation]\n",
    "        y = y[permutation]\n",
    "\n",
    "        # Process mini-batches\n",
    "        for i in range(0, num_train, batch_size):\n",
    "            # Get mini-batch\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "\n",
    "            # Compute scores\n",
    "            scores = np.dot(X_batch, W) + b\n",
    "\n",
    "            # Compute softmax probabilities\n",
    "            probs = softmax(scores, axis=1)\n",
    "\n",
    "            # Compute negative log-likelihood loss with L2 regularization\n",
    "            loss = -np.sum(np.log(probs[np.arange(batch_size), y_batch])) / batch_size\n",
    "            loss += 0.5 * reg_param * np.sum(W**)\n",
    "            dscores = probs\n",
    "        dscores[np.arange(batch_size), y_batch] -= 1\n",
    "        dscores /= batch_size\n",
    "\n",
    "        # Update biases\n",
    "        db = np.sum(dscores, axis=0)\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        # Update weights with regularization\n",
    "        dW = np.dot(X_batch.T, dscores)\n",
    "        dW += reg_param * W\n",
    "        W -= learning_rate * dW\n",
    "\n",
    "    # Compute training loss and error rate for this epoch\n",
    "    train_scores = np.dot(X, W) + b\n",
    "    train_probs = softmax(train_scores, axis=1)\n",
    "    train_loss = -np.sum(np.log(train_probs[np.arange(num_train), y])) / num_train\n",
    "    train_loss += 0.5 * reg_param * np.sum(W**2)\n",
    "    train_losses.append(train_loss)\n",
    "    train_preds = np.argmax(train_probs, axis=1)\n",
    "    train_error_rate = np.mean(train_preds != y)\n",
    "    train_error_rates.append(train_error_rate)\n",
    "\n",
    "    # Compute validation loss and error rate for this epoch\n",
    "    val_scores = np.dot(X_val, W) + b\n",
    "    val_probs = softmax(val_scores, axis=1)\n",
    "    val_loss = -np.sum(np.log(val_probs[np.arange(num_val), y_val])) / num_val\n",
    "    val_loss += 0.5 * reg_param * np.sum(W**2)\n",
    "    val_losses.append(val_loss)\n",
    "    val_preds = np.argmax(val_probs, axis=1)\n",
    "    val_error_rate = np.mean(val_preds != y_val)\n",
    "    val_error_rates.append(val_error_rate)\n",
    "\n",
    "    # Check if validation loss has stopped decreasing\n",
    "    if epoch > 0 and val_losses[epoch] > val_losses[epoch-1]:\n",
    "        break\n",
    "\n",
    "    return W, b, train_losses, val_losses, train_error_rates, val_error_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax, logsumexp\n",
    "\n",
    "def predict(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        if sigmoid(w, X[i], b) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "\n",
    "def sigmoid(w,x,b):\n",
    "    return 1/(1+math.exp(-(np.dot(x,w)+b)))\n",
    "\n",
    "def sig_pred(w,X,b):\n",
    "    a=[]\n",
    "    for x in X:\n",
    "        a.append(sigmoid(w,x,b))\n",
    "#         a.append(1/(1+math.exp(-(np.dot(x,w)+b))))\n",
    "    return a\n",
    "    \n",
    "    \n",
    "from math import log10\n",
    "def compute_log_loss(A,B):\n",
    "    n = len(A)\n",
    "    res = 0\n",
    "    for l in zip(A,B):\n",
    "        res += l[0] * log10(l[1]) + (1 - l[0]) * log10(1 - l[1])                   \n",
    "    loss = (-1 * res) / n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X_train,X_test,y_train,y_test,n,w,b):\n",
    "    del_w=np.zeros_like(X_train[0])\n",
    "    del_b=0\n",
    "    wl=[]\n",
    "    bl=[]\n",
    "    log_loss_train=[]\n",
    "    log_loss_test=[]\n",
    "    train_error_rates=[]\n",
    "    test_error_rates=[]\n",
    "    for epoch in range(n):\n",
    "        for m in range(1000):\n",
    "            i = np.random.choice(len(X_train))\n",
    "            del_w=del_w+X_train[i]*(y_train[i]-sigmoid(w,X_train[i],b))\n",
    "            del_b=del_b+(y_train[i]-sigmoid(w,X_train[i],b))\n",
    "        w=(1-(alpha/n))*w+alpha*del_w\n",
    "        b=b+alpha*del_b\n",
    "        wl.append(w)\n",
    "        bl.append(b)\n",
    "        log_loss_train.append(compute_log_loss(y_train,sig_pred(w,X_train,b)))\n",
    "        log_loss_test.append(compute_log_loss(y_test,sig_pred(w,X_test,b)))\n",
    "        \n",
    "        train_error_rates.append(accuracy_score(y_train, pred(w,b, X_train)))\n",
    "        test_error_rates.append(accuracy_score(y_test, pred(w,b, X_test)))\n",
    "        \n",
    "    return log_loss_train,log_loss_test,wl,bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.02, NNZs: 708, Bias: -0.156700, T: 42000, Avg. loss: 0.187888\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.33, NNZs: 708, Bias: -0.214329, T: 84000, Avg. loss: 0.098149\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.53, NNZs: 708, Bias: -0.257526, T: 126000, Avg. loss: 0.079925\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.69, NNZs: 708, Bias: -0.292829, T: 168000, Avg. loss: 0.070613\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.81, NNZs: 708, Bias: -0.325020, T: 210000, Avg. loss: 0.064682\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.91, NNZs: 708, Bias: -0.354095, T: 252000, Avg. loss: 0.060466\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.01, NNZs: 708, Bias: -0.379614, T: 294000, Avg. loss: 0.057291\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.09, NNZs: 708, Bias: -0.404044, T: 336000, Avg. loss: 0.054764\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.16, NNZs: 708, Bias: -0.427389, T: 378000, Avg. loss: 0.052715\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.23, NNZs: 708, Bias: -0.447654, T: 420000, Avg. loss: 0.050982\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.29, NNZs: 708, Bias: -0.469733, T: 462000, Avg. loss: 0.049541\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.35, NNZs: 708, Bias: -0.490713, T: 504000, Avg. loss: 0.048269\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.40, NNZs: 708, Bias: -0.508880, T: 546000, Avg. loss: 0.047151\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.45, NNZs: 708, Bias: -0.528312, T: 588000, Avg. loss: 0.046191\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.50, NNZs: 708, Bias: -0.547834, T: 630000, Avg. loss: 0.045299\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.55, NNZs: 708, Bias: -0.564284, T: 672000, Avg. loss: 0.044519\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.59, NNZs: 708, Bias: -0.582094, T: 714000, Avg. loss: 0.043813\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.63, NNZs: 708, Bias: -0.598797, T: 756000, Avg. loss: 0.043161\n",
      "Total training time: 1.81 seconds.\n",
      "Convergence after 18 epochs took 1.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.96, NNZs: 708, Bias: -0.046455, T: 42000, Avg. loss: 0.169701\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.29, NNZs: 708, Bias: -0.049182, T: 84000, Avg. loss: 0.088929\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.50, NNZs: 708, Bias: -0.053100, T: 126000, Avg. loss: 0.071302\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.65, NNZs: 708, Bias: -0.056162, T: 168000, Avg. loss: 0.062776\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.78, NNZs: 708, Bias: -0.061827, T: 210000, Avg. loss: 0.057551\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.88, NNZs: 708, Bias: -0.065461, T: 252000, Avg. loss: 0.053962\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.97, NNZs: 708, Bias: -0.069017, T: 294000, Avg. loss: 0.051302\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.05, NNZs: 708, Bias: -0.073484, T: 336000, Avg. loss: 0.049235\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.13, NNZs: 708, Bias: -0.077180, T: 378000, Avg. loss: 0.047577\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.19, NNZs: 708, Bias: -0.082055, T: 420000, Avg. loss: 0.046193\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.25, NNZs: 708, Bias: -0.085490, T: 462000, Avg. loss: 0.045055\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.31, NNZs: 708, Bias: -0.089599, T: 504000, Avg. loss: 0.044067\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.36, NNZs: 708, Bias: -0.093077, T: 546000, Avg. loss: 0.043205\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.41, NNZs: 708, Bias: -0.098186, T: 588000, Avg. loss: 0.042452\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.45, NNZs: 708, Bias: -0.102078, T: 630000, Avg. loss: 0.041783\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.50, NNZs: 708, Bias: -0.106151, T: 672000, Avg. loss: 0.041184\n",
      "Total training time: 1.57 seconds.\n",
      "Convergence after 16 epochs took 1.57 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.91, NNZs: 708, Bias: -0.152553, T: 42000, Avg. loss: 0.250428\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.27, NNZs: 708, Bias: -0.220173, T: 84000, Avg. loss: 0.162326\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.51, NNZs: 708, Bias: -0.274136, T: 126000, Avg. loss: 0.138085\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.69, NNZs: 708, Bias: -0.318386, T: 168000, Avg. loss: 0.125625\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.84, NNZs: 708, Bias: -0.360614, T: 210000, Avg. loss: 0.117814\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.96, NNZs: 708, Bias: -0.396648, T: 252000, Avg. loss: 0.112381\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.07, NNZs: 708, Bias: -0.429359, T: 294000, Avg. loss: 0.108299\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.16, NNZs: 708, Bias: -0.460450, T: 336000, Avg. loss: 0.105101\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.24, NNZs: 708, Bias: -0.489098, T: 378000, Avg. loss: 0.102509\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.32, NNZs: 708, Bias: -0.517701, T: 420000, Avg. loss: 0.100359\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.39, NNZs: 708, Bias: -0.543387, T: 462000, Avg. loss: 0.098529\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.46, NNZs: 708, Bias: -0.570614, T: 504000, Avg. loss: 0.096920\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.51, NNZs: 708, Bias: -0.593358, T: 546000, Avg. loss: 0.095617\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.57, NNZs: 708, Bias: -0.616677, T: 588000, Avg. loss: 0.094374\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.62, NNZs: 708, Bias: -0.637205, T: 630000, Avg. loss: 0.093295\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.67, NNZs: 708, Bias: -0.658031, T: 672000, Avg. loss: 0.092320\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.72, NNZs: 708, Bias: -0.680150, T: 714000, Avg. loss: 0.091453\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.76, NNZs: 708, Bias: -0.701211, T: 756000, Avg. loss: 0.090638\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.80, NNZs: 708, Bias: -0.720460, T: 798000, Avg. loss: 0.089911\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.84, NNZs: 708, Bias: -0.739955, T: 840000, Avg. loss: 0.089230\n",
      "Total training time: 2.05 seconds.\n",
      "Convergence after 20 epochs took 2.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.90, NNZs: 708, Bias: -0.153663, T: 42000, Avg. loss: 0.252922\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.24, NNZs: 708, Bias: -0.224354, T: 84000, Avg. loss: 0.169413\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.46, NNZs: 708, Bias: -0.282883, T: 126000, Avg. loss: 0.147067\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.63, NNZs: 708, Bias: -0.332738, T: 168000, Avg. loss: 0.135474\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.77, NNZs: 708, Bias: -0.378842, T: 210000, Avg. loss: 0.128201\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.88, NNZs: 708, Bias: -0.418916, T: 252000, Avg. loss: 0.123053\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.98, NNZs: 708, Bias: -0.458320, T: 294000, Avg. loss: 0.119191\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.07, NNZs: 708, Bias: -0.494956, T: 336000, Avg. loss: 0.116126\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.15, NNZs: 708, Bias: -0.530528, T: 378000, Avg. loss: 0.113674\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.22, NNZs: 708, Bias: -0.562469, T: 420000, Avg. loss: 0.111583\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.28, NNZs: 708, Bias: -0.596946, T: 462000, Avg. loss: 0.109862\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.34, NNZs: 708, Bias: -0.627127, T: 504000, Avg. loss: 0.108334\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.40, NNZs: 708, Bias: -0.658918, T: 546000, Avg. loss: 0.106961\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.45, NNZs: 708, Bias: -0.687014, T: 588000, Avg. loss: 0.105803\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.50, NNZs: 708, Bias: -0.716964, T: 630000, Avg. loss: 0.104724\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.54, NNZs: 708, Bias: -0.745775, T: 672000, Avg. loss: 0.103741\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.58, NNZs: 708, Bias: -0.772810, T: 714000, Avg. loss: 0.102890\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.63, NNZs: 708, Bias: -0.797608, T: 756000, Avg. loss: 0.102071\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.66, NNZs: 708, Bias: -0.823414, T: 798000, Avg. loss: 0.101319\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.70, NNZs: 708, Bias: -0.850127, T: 840000, Avg. loss: 0.100621\n",
      "Total training time: 2.62 seconds.\n",
      "Convergence after 20 epochs took 2.62 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.88, NNZs: 708, Bias: -0.107442, T: 42000, Avg. loss: 0.232032\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.24, NNZs: 708, Bias: -0.145203, T: 84000, Avg. loss: 0.150484\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.48, NNZs: 708, Bias: -0.171746, T: 126000, Avg. loss: 0.126380\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.67, NNZs: 708, Bias: -0.194727, T: 168000, Avg. loss: 0.113600\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.83, NNZs: 708, Bias: -0.217541, T: 210000, Avg. loss: 0.105351\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.96, NNZs: 708, Bias: -0.237094, T: 252000, Avg. loss: 0.099502\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.07, NNZs: 708, Bias: -0.255671, T: 294000, Avg. loss: 0.095055\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.18, NNZs: 708, Bias: -0.271705, T: 336000, Avg. loss: 0.091565\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.27, NNZs: 708, Bias: -0.289149, T: 378000, Avg. loss: 0.088692\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.36, NNZs: 708, Bias: -0.304930, T: 420000, Avg. loss: 0.086326\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.43, NNZs: 708, Bias: -0.319108, T: 462000, Avg. loss: 0.084310\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.51, NNZs: 708, Bias: -0.334325, T: 504000, Avg. loss: 0.082568\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.58, NNZs: 708, Bias: -0.348096, T: 546000, Avg. loss: 0.081038\n",
      "Total training time: 2.78 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.64, NNZs: 708, Bias: -0.362242, T: 588000, Avg. loss: 0.079711\n",
      "Total training time: 2.99 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.70, NNZs: 708, Bias: -0.376082, T: 630000, Avg. loss: 0.078519\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.76, NNZs: 708, Bias: -0.388219, T: 672000, Avg. loss: 0.077448\n",
      "Total training time: 3.41 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.81, NNZs: 708, Bias: -0.401515, T: 714000, Avg. loss: 0.076481\n",
      "Total training time: 3.64 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.86, NNZs: 708, Bias: -0.412822, T: 756000, Avg. loss: 0.075596\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.91, NNZs: 708, Bias: -0.427347, T: 798000, Avg. loss: 0.074811\n",
      "Total training time: 4.07 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.96, NNZs: 708, Bias: -0.439901, T: 840000, Avg. loss: 0.074072\n",
      "Total training time: 4.28 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 3.00, NNZs: 708, Bias: -0.451498, T: 882000, Avg. loss: 0.073398\n",
      "Total training time: 4.51 seconds.\n",
      "Convergence after 21 epochs took 4.51 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 708, Bias: -0.109583, T: 42000, Avg. loss: 0.258263\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.09, NNZs: 708, Bias: -0.140710, T: 84000, Avg. loss: 0.192295\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.33, NNZs: 708, Bias: -0.163099, T: 126000, Avg. loss: 0.168575\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.52, NNZs: 708, Bias: -0.180643, T: 168000, Avg. loss: 0.155170\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.68, NNZs: 708, Bias: -0.194954, T: 210000, Avg. loss: 0.146340\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.82, NNZs: 708, Bias: -0.207573, T: 252000, Avg. loss: 0.139986\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.94, NNZs: 708, Bias: -0.217132, T: 294000, Avg. loss: 0.135158\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.05, NNZs: 708, Bias: -0.226722, T: 336000, Avg. loss: 0.131339\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.15, NNZs: 708, Bias: -0.235854, T: 378000, Avg. loss: 0.128222\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.23, NNZs: 708, Bias: -0.242851, T: 420000, Avg. loss: 0.125607\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.32, NNZs: 708, Bias: -0.251288, T: 462000, Avg. loss: 0.123416\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.39, NNZs: 708, Bias: -0.258234, T: 504000, Avg. loss: 0.121510\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.46, NNZs: 708, Bias: -0.264225, T: 546000, Avg. loss: 0.119834\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.53, NNZs: 708, Bias: -0.266927, T: 588000, Avg. loss: 0.118301\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.59, NNZs: 708, Bias: -0.274311, T: 630000, Avg. loss: 0.117043\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.65, NNZs: 708, Bias: -0.279314, T: 672000, Avg. loss: 0.115835\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.71, NNZs: 708, Bias: -0.283354, T: 714000, Avg. loss: 0.114748\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.76, NNZs: 708, Bias: -0.288624, T: 756000, Avg. loss: 0.113776\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.81, NNZs: 708, Bias: -0.292357, T: 798000, Avg. loss: 0.112866\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.86, NNZs: 708, Bias: -0.296375, T: 840000, Avg. loss: 0.112046\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.91, NNZs: 708, Bias: -0.300254, T: 882000, Avg. loss: 0.111269\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.95, NNZs: 708, Bias: -0.304903, T: 924000, Avg. loss: 0.110550\n",
      "Total training time: 2.93 seconds.\n",
      "Convergence after 22 epochs took 2.93 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.97, NNZs: 708, Bias: -0.136826, T: 42000, Avg. loss: 0.217267\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.34, NNZs: 708, Bias: -0.189202, T: 84000, Avg. loss: 0.121888\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.58, NNZs: 708, Bias: -0.230388, T: 126000, Avg. loss: 0.098129\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.75, NNZs: 708, Bias: -0.262128, T: 168000, Avg. loss: 0.086561\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.90, NNZs: 708, Bias: -0.294189, T: 210000, Avg. loss: 0.079511\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.01, NNZs: 708, Bias: -0.318468, T: 252000, Avg. loss: 0.074694\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.12, NNZs: 708, Bias: -0.343313, T: 294000, Avg. loss: 0.071136\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.21, NNZs: 708, Bias: -0.364787, T: 336000, Avg. loss: 0.068371\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.29, NNZs: 708, Bias: -0.384794, T: 378000, Avg. loss: 0.066118\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.36, NNZs: 708, Bias: -0.406101, T: 420000, Avg. loss: 0.064327\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.42, NNZs: 708, Bias: -0.426242, T: 462000, Avg. loss: 0.062771\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.49, NNZs: 708, Bias: -0.443740, T: 504000, Avg. loss: 0.061448\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.54, NNZs: 708, Bias: -0.462255, T: 546000, Avg. loss: 0.060301\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.59, NNZs: 708, Bias: -0.478775, T: 588000, Avg. loss: 0.059284\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.64, NNZs: 708, Bias: -0.496939, T: 630000, Avg. loss: 0.058394\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.69, NNZs: 708, Bias: -0.512125, T: 672000, Avg. loss: 0.057592\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.74, NNZs: 708, Bias: -0.528837, T: 714000, Avg. loss: 0.056872\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.78, NNZs: 708, Bias: -0.544928, T: 756000, Avg. loss: 0.056208\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.82, NNZs: 708, Bias: -0.559754, T: 798000, Avg. loss: 0.055615\n",
      "Total training time: 2.11 seconds.\n",
      "Convergence after 19 epochs took 2.11 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.95, NNZs: 708, Bias: -0.100154, T: 42000, Avg. loss: 0.208945\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.29, NNZs: 708, Bias: -0.127254, T: 84000, Avg. loss: 0.121946\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.52, NNZs: 708, Bias: -0.146184, T: 126000, Avg. loss: 0.101223\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.68, NNZs: 708, Bias: -0.162635, T: 168000, Avg. loss: 0.091181\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.82, NNZs: 708, Bias: -0.175425, T: 210000, Avg. loss: 0.085059\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.93, NNZs: 708, Bias: -0.188724, T: 252000, Avg. loss: 0.080847\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.02, NNZs: 708, Bias: -0.200292, T: 294000, Avg. loss: 0.077752\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.11, NNZs: 708, Bias: -0.209103, T: 336000, Avg. loss: 0.075361\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.18, NNZs: 708, Bias: -0.216247, T: 378000, Avg. loss: 0.073397\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.25, NNZs: 708, Bias: -0.226845, T: 420000, Avg. loss: 0.071835\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.32, NNZs: 708, Bias: -0.234369, T: 462000, Avg. loss: 0.070475\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.38, NNZs: 708, Bias: -0.243349, T: 504000, Avg. loss: 0.069323\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.43, NNZs: 708, Bias: -0.250771, T: 546000, Avg. loss: 0.068305\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.48, NNZs: 708, Bias: -0.258277, T: 588000, Avg. loss: 0.067420\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.53, NNZs: 708, Bias: -0.265328, T: 630000, Avg. loss: 0.066627\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.57, NNZs: 708, Bias: -0.272590, T: 672000, Avg. loss: 0.065915\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.61, NNZs: 708, Bias: -0.280008, T: 714000, Avg. loss: 0.065264\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.66, NNZs: 708, Bias: -0.286355, T: 756000, Avg. loss: 0.064685\n",
      "Total training time: 2.43 seconds.\n",
      "Convergence after 18 epochs took 2.43 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.74, NNZs: 708, Bias: -0.197766, T: 42000, Avg. loss: 0.313465\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.08, NNZs: 708, Bias: -0.318419, T: 84000, Avg. loss: 0.242723\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.31, NNZs: 708, Bias: -0.425916, T: 126000, Avg. loss: 0.217257\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.48, NNZs: 708, Bias: -0.523808, T: 168000, Avg. loss: 0.203314\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.62, NNZs: 708, Bias: -0.614519, T: 210000, Avg. loss: 0.194100\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.73, NNZs: 708, Bias: -0.701133, T: 252000, Avg. loss: 0.187414\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.83, NNZs: 708, Bias: -0.781043, T: 294000, Avg. loss: 0.182223\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.91, NNZs: 708, Bias: -0.859734, T: 336000, Avg. loss: 0.177976\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.99, NNZs: 708, Bias: -0.935669, T: 378000, Avg. loss: 0.174475\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.06, NNZs: 708, Bias: -1.008144, T: 420000, Avg. loss: 0.171420\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.12, NNZs: 708, Bias: -1.077467, T: 462000, Avg. loss: 0.168708\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.17, NNZs: 708, Bias: -1.148004, T: 504000, Avg. loss: 0.166334\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.22, NNZs: 708, Bias: -1.213104, T: 546000, Avg. loss: 0.164187\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.27, NNZs: 708, Bias: -1.279690, T: 588000, Avg. loss: 0.162241\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.31, NNZs: 708, Bias: -1.342077, T: 630000, Avg. loss: 0.160425\n",
      "Total training time: 3.19 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.35, NNZs: 708, Bias: -1.404283, T: 672000, Avg. loss: 0.158723\n",
      "Total training time: 3.43 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.39, NNZs: 708, Bias: -1.464563, T: 714000, Avg. loss: 0.157248\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.42, NNZs: 708, Bias: -1.524104, T: 756000, Avg. loss: 0.155781\n",
      "Total training time: 3.92 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.46, NNZs: 708, Bias: -1.581902, T: 798000, Avg. loss: 0.154437\n",
      "Total training time: 4.18 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.49, NNZs: 708, Bias: -1.637449, T: 840000, Avg. loss: 0.153176\n",
      "Total training time: 4.41 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.52, NNZs: 708, Bias: -1.693184, T: 882000, Avg. loss: 0.151971\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.55, NNZs: 708, Bias: -1.746305, T: 924000, Avg. loss: 0.150847\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.58, NNZs: 708, Bias: -1.799903, T: 966000, Avg. loss: 0.149762\n",
      "Total training time: 5.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.60, NNZs: 708, Bias: -1.853226, T: 1008000, Avg. loss: 0.148720\n",
      "Total training time: 5.24 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.63, NNZs: 708, Bias: -1.904374, T: 1050000, Avg. loss: 0.147761\n",
      "Total training time: 5.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.65, NNZs: 708, Bias: -1.953954, T: 1092000, Avg. loss: 0.146816\n",
      "Total training time: 5.65 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.68, NNZs: 708, Bias: -2.004755, T: 1134000, Avg. loss: 0.145911\n",
      "Total training time: 5.87 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.70, NNZs: 708, Bias: -2.051532, T: 1176000, Avg. loss: 0.145074\n",
      "Total training time: 6.11 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.72, NNZs: 708, Bias: -2.097926, T: 1218000, Avg. loss: 0.144228\n",
      "Total training time: 6.34 seconds.\n",
      "Convergence after 29 epochs took 6.34 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.79, NNZs: 708, Bias: -0.137100, T: 42000, Avg. loss: 0.273252\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.10, NNZs: 708, Bias: -0.202415, T: 84000, Avg. loss: 0.206559\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.33, NNZs: 708, Bias: -0.258779, T: 126000, Avg. loss: 0.184151\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.51, NNZs: 708, Bias: -0.307266, T: 168000, Avg. loss: 0.170986\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.66, NNZs: 708, Bias: -0.354665, T: 210000, Avg. loss: 0.162025\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.80, NNZs: 708, Bias: -0.398149, T: 252000, Avg. loss: 0.155462\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.91, NNZs: 708, Bias: -0.441288, T: 294000, Avg. loss: 0.150373\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.02, NNZs: 708, Bias: -0.478851, T: 336000, Avg. loss: 0.146338\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.11, NNZs: 708, Bias: -0.519089, T: 378000, Avg. loss: 0.143044\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.20, NNZs: 708, Bias: -0.555808, T: 420000, Avg. loss: 0.140252\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.28, NNZs: 708, Bias: -0.591687, T: 462000, Avg. loss: 0.137852\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.35, NNZs: 708, Bias: -0.625083, T: 504000, Avg. loss: 0.135850\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.42, NNZs: 708, Bias: -0.659821, T: 546000, Avg. loss: 0.134056\n",
      "Total training time: 3.09 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.48, NNZs: 708, Bias: -0.691063, T: 588000, Avg. loss: 0.132469\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.54, NNZs: 708, Bias: -0.723145, T: 630000, Avg. loss: 0.131052\n",
      "Total training time: 3.58 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.60, NNZs: 708, Bias: -0.754517, T: 672000, Avg. loss: 0.129765\n",
      "Total training time: 3.81 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.65, NNZs: 708, Bias: -0.784337, T: 714000, Avg. loss: 0.128617\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.70, NNZs: 708, Bias: -0.814759, T: 756000, Avg. loss: 0.127558\n",
      "Total training time: 4.26 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.75, NNZs: 708, Bias: -0.842857, T: 798000, Avg. loss: 0.126607\n",
      "Total training time: 4.47 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.79, NNZs: 708, Bias: -0.872210, T: 840000, Avg. loss: 0.125705\n",
      "Total training time: 4.69 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.83, NNZs: 708, Bias: -0.899910, T: 882000, Avg. loss: 0.124882\n",
      "Total training time: 4.90 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.87, NNZs: 708, Bias: -0.928635, T: 924000, Avg. loss: 0.124081\n",
      "Total training time: 5.11 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.91, NNZs: 708, Bias: -0.954171, T: 966000, Avg. loss: 0.123416\n",
      "Total training time: 5.32 seconds.\n",
      "Convergence after 23 epochs took 5.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   31.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 708, Bias: -0.755514, T: 42000, Avg. loss: 0.414341\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.219951, T: 84000, Avg. loss: 0.353920\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.522232, T: 126000, Avg. loss: 0.329005\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.728232, T: 168000, Avg. loss: 0.317186\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.872416, T: 210000, Avg. loss: 0.311205\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.975408, T: 252000, Avg. loss: 0.308106\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.048989, T: 294000, Avg. loss: 0.306334\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.103006, T: 336000, Avg. loss: 0.305403\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.143317, T: 378000, Avg. loss: 0.304758\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.172953, T: 420000, Avg. loss: 0.304365\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.194496, T: 462000, Avg. loss: 0.304281\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.210810, T: 504000, Avg. loss: 0.304144\n",
      "Total training time: 1.27 seconds.\n",
      "Convergence after 12 epochs took 1.27 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.10, NNZs: 708, Bias: -0.669948, T: 42000, Avg. loss: 0.393374\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 0.08, NNZs: 708, Bias: -1.080408, T: 84000, Avg. loss: 0.353343\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.347578, T: 126000, Avg. loss: 0.338714\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.526506, T: 168000, Avg. loss: 0.332824\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.651029, T: 210000, Avg. loss: 0.330413\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.738225, T: 252000, Avg. loss: 0.329416\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.799965, T: 294000, Avg. loss: 0.329142\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.843928, T: 336000, Avg. loss: 0.329129\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.875170, T: 378000, Avg. loss: 0.329165\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.898146, T: 420000, Avg. loss: 0.329277\n",
      "Total training time: 0.95 seconds.\n",
      "Convergence after 10 epochs took 0.95 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 708, Bias: -0.743540, T: 42000, Avg. loss: 0.421473\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.198264, T: 84000, Avg. loss: 0.364518\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.494864, T: 126000, Avg. loss: 0.341248\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.695671, T: 168000, Avg. loss: 0.330411\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.836731, T: 210000, Avg. loss: 0.325120\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.936592, T: 252000, Avg. loss: 0.322442\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.008588, T: 294000, Avg. loss: 0.320942\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.060328, T: 336000, Avg. loss: 0.320127\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.098182, T: 378000, Avg. loss: 0.319674\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.126170, T: 420000, Avg. loss: 0.319438\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.147143, T: 462000, Avg. loss: 0.319288\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.162946, T: 504000, Avg. loss: 0.319136\n",
      "Total training time: 1.18 seconds.\n",
      "Convergence after 12 epochs took 1.18 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 708, Bias: -0.742797, T: 42000, Avg. loss: 0.421526\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.197607, T: 84000, Avg. loss: 0.364381\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.494131, T: 126000, Avg. loss: 0.341211\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.695278, T: 168000, Avg. loss: 0.330365\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.835948, T: 210000, Avg. loss: 0.325095\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.936014, T: 252000, Avg. loss: 0.322197\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.007523, T: 294000, Avg. loss: 0.320809\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.058866, T: 336000, Avg. loss: 0.320045\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.097437, T: 378000, Avg. loss: 0.319531\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.125360, T: 420000, Avg. loss: 0.319292\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.145851, T: 462000, Avg. loss: 0.319137\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.161393, T: 504000, Avg. loss: 0.319097\n",
      "Total training time: 1.16 seconds.\n",
      "Convergence after 12 epochs took 1.16 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 708, Bias: -0.731346, T: 42000, Avg. loss: 0.407171\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.182194, T: 84000, Avg. loss: 0.353893\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.475746, T: 126000, Avg. loss: 0.332599\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.675470, T: 168000, Avg. loss: 0.322877\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.816606, T: 210000, Avg. loss: 0.318184\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.917527, T: 252000, Avg. loss: 0.315874\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.990086, T: 294000, Avg. loss: 0.314745\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.042716, T: 336000, Avg. loss: 0.314128\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.081386, T: 378000, Avg. loss: 0.313764\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.110120, T: 420000, Avg. loss: 0.313619\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.131246, T: 462000, Avg. loss: 0.313527\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.146790, T: 504000, Avg. loss: 0.313518\n",
      "Total training time: 1.18 seconds.\n",
      "Convergence after 12 epochs took 1.18 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 708, Bias: -0.754271, T: 42000, Avg. loss: 0.400614\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.220046, T: 84000, Avg. loss: 0.343141\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.527526, T: 126000, Avg. loss: 0.319559\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.738666, T: 168000, Avg. loss: 0.308586\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.888266, T: 210000, Avg. loss: 0.303124\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.996716, T: 252000, Avg. loss: 0.300292\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.075765, T: 294000, Avg. loss: 0.298760\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.133973, T: 336000, Avg. loss: 0.297983\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.177920, T: 378000, Avg. loss: 0.297466\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.210792, T: 420000, Avg. loss: 0.297204\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.235813, T: 462000, Avg. loss: 0.297157\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.254721, T: 504000, Avg. loss: 0.297080\n",
      "Total training time: 1.17 seconds.\n",
      "Convergence after 12 epochs took 1.17 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 708, Bias: -0.746775, T: 42000, Avg. loss: 0.410983\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.204340, T: 84000, Avg. loss: 0.353995\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.504993, T: 126000, Avg. loss: 0.330763\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.708389, T: 168000, Avg. loss: 0.319944\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.852844, T: 210000, Avg. loss: 0.314656\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.954927, T: 252000, Avg. loss: 0.311863\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.029313, T: 294000, Avg. loss: 0.310402\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.083202, T: 336000, Avg. loss: 0.309625\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.123058, T: 378000, Avg. loss: 0.309114\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.152284, T: 420000, Avg. loss: 0.308982\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.175328, T: 462000, Avg. loss: 0.308728\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.191851, T: 504000, Avg. loss: 0.308639\n",
      "Total training time: 2.35 seconds.\n",
      "Convergence after 12 epochs took 2.35 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.09, NNZs: 708, Bias: -0.716731, T: 42000, Avg. loss: 0.407144\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.155880, T: 84000, Avg. loss: 0.356933\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.442831, T: 126000, Avg. loss: 0.337234\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.637517, T: 168000, Avg. loss: 0.328380\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.772587, T: 210000, Avg. loss: 0.324156\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.868333, T: 252000, Avg. loss: 0.322229\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.937173, T: 294000, Avg. loss: 0.321244\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.987383, T: 336000, Avg. loss: 0.320722\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.022902, T: 378000, Avg. loss: 0.320571\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.050620, T: 420000, Avg. loss: 0.320416\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.04, NNZs: 708, Bias: -2.069896, T: 462000, Avg. loss: 0.320390\n",
      "Total training time: 2.45 seconds.\n",
      "Convergence after 11 epochs took 2.45 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 708, Bias: -0.767728, T: 42000, Avg. loss: 0.429717\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.05, NNZs: 708, Bias: -1.236400, T: 84000, Avg. loss: 0.366874\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.543482, T: 126000, Avg. loss: 0.340570\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.752009, T: 168000, Avg. loss: 0.328099\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.02, NNZs: 708, Bias: -1.898404, T: 210000, Avg. loss: 0.321771\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.002700, T: 252000, Avg. loss: 0.318421\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.078037, T: 294000, Avg. loss: 0.316504\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.132585, T: 336000, Avg. loss: 0.315399\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.173009, T: 378000, Avg. loss: 0.314827\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.203762, T: 420000, Avg. loss: 0.314440\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.226289, T: 462000, Avg. loss: 0.314175\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.243051, T: 504000, Avg. loss: 0.314038\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.02, NNZs: 708, Bias: -2.255487, T: 546000, Avg. loss: 0.313915\n",
      "Total training time: 2.77 seconds.\n",
      "Convergence after 13 epochs took 2.77 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 708, Bias: -0.737871, T: 42000, Avg. loss: 0.417360\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.06, NNZs: 708, Bias: -1.190099, T: 84000, Avg. loss: 0.361969\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.04, NNZs: 708, Bias: -1.486115, T: 126000, Avg. loss: 0.339616\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.686190, T: 168000, Avg. loss: 0.329341\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.825688, T: 210000, Avg. loss: 0.324330\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.925297, T: 252000, Avg. loss: 0.321792\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.03, NNZs: 708, Bias: -1.997855, T: 294000, Avg. loss: 0.320379\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.049997, T: 336000, Avg. loss: 0.319750\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.088909, T: 378000, Avg. loss: 0.319338\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.117818, T: 420000, Avg. loss: 0.319094\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.139040, T: 462000, Avg. loss: 0.318934\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.03, NNZs: 708, Bias: -2.154642, T: 504000, Avg. loss: 0.318897\n",
      "Total training time: 2.86 seconds.\n",
      "Convergence after 12 epochs took 2.86 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   17.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 0.01, NNZs: 708, Bias: -1.020191, T: 42000, Avg. loss: 0.489085\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.494316, T: 84000, Avg. loss: 0.368210\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.752505, T: 126000, Avg. loss: 0.338307\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.908045, T: 168000, Avg. loss: 0.328486\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.007593, T: 210000, Avg. loss: 0.324682\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.072528, T: 252000, Avg. loss: 0.323167\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.115139, T: 294000, Avg. loss: 0.322387\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.144325, T: 336000, Avg. loss: 0.322122\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.165119, T: 378000, Avg. loss: 0.321962\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.179039, T: 420000, Avg. loss: 0.321863\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.188021, T: 462000, Avg. loss: 0.321840\n",
      "Total training time: 2.12 seconds.\n",
      "Convergence after 11 epochs took 2.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -0.979266, T: 42000, Avg. loss: 0.494001\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.429488, T: 84000, Avg. loss: 0.385495\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.673025, T: 126000, Avg. loss: 0.359501\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.814739, T: 168000, Avg. loss: 0.351501\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.904120, T: 210000, Avg. loss: 0.348648\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.960575, T: 252000, Avg. loss: 0.347545\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.997207, T: 294000, Avg. loss: 0.347134\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.020410, T: 336000, Avg. loss: 0.346944\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.035488, T: 378000, Avg. loss: 0.346887\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.045750, T: 420000, Avg. loss: 0.346869\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.052436, T: 462000, Avg. loss: 0.346858\n",
      "Total training time: 1.10 seconds.\n",
      "Convergence after 11 epochs took 1.10 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.014498, T: 42000, Avg. loss: 0.491111\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.482655, T: 84000, Avg. loss: 0.372457\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.738682, T: 126000, Avg. loss: 0.343266\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.891273, T: 168000, Avg. loss: 0.333764\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.989886, T: 210000, Avg. loss: 0.330123\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.053256, T: 252000, Avg. loss: 0.328641\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.095354, T: 294000, Avg. loss: 0.327998\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.123042, T: 336000, Avg. loss: 0.327700\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.141465, T: 378000, Avg. loss: 0.327589\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.154406, T: 420000, Avg. loss: 0.327503\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.163474, T: 462000, Avg. loss: 0.327483\n",
      "Total training time: 1.09 seconds.\n",
      "Convergence after 11 epochs took 1.09 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.013056, T: 42000, Avg. loss: 0.491349\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.482073, T: 84000, Avg. loss: 0.372697\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.737726, T: 126000, Avg. loss: 0.343492\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.890840, T: 168000, Avg. loss: 0.334031\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.988423, T: 210000, Avg. loss: 0.330392\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.052456, T: 252000, Avg. loss: 0.328892\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.093658, T: 294000, Avg. loss: 0.328222\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.121106, T: 336000, Avg. loss: 0.327969\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.140651, T: 378000, Avg. loss: 0.327831\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.153225, T: 420000, Avg. loss: 0.327766\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.161742, T: 462000, Avg. loss: 0.327735\n",
      "Total training time: 1.08 seconds.\n",
      "Convergence after 11 epochs took 1.08 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.018291, T: 42000, Avg. loss: 0.486366\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.493050, T: 84000, Avg. loss: 0.366758\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.750891, T: 126000, Avg. loss: 0.337162\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.907180, T: 168000, Avg. loss: 0.327489\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.007886, T: 210000, Avg. loss: 0.323748\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.073644, T: 252000, Avg. loss: 0.322217\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.117744, T: 294000, Avg. loss: 0.321568\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.147245, T: 336000, Avg. loss: 0.321253\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.167325, T: 378000, Avg. loss: 0.321129\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.180954, T: 420000, Avg. loss: 0.321076\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.190322, T: 462000, Avg. loss: 0.321021\n",
      "Total training time: 1.06 seconds.\n",
      "Convergence after 11 epochs took 1.06 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.043603, T: 42000, Avg. loss: 0.477546\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.532699, T: 84000, Avg. loss: 0.351235\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.805853, T: 126000, Avg. loss: 0.319091\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.972909, T: 168000, Avg. loss: 0.308221\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.081814, T: 210000, Avg. loss: 0.303894\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.154928, T: 252000, Avg. loss: 0.302045\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.204663, T: 294000, Avg. loss: 0.301152\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.238885, T: 336000, Avg. loss: 0.300784\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.263194, T: 378000, Avg. loss: 0.300555\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.280060, T: 420000, Avg. loss: 0.300472\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.292492, T: 462000, Avg. loss: 0.300431\n",
      "Total training time: 1.17 seconds.\n",
      "Convergence after 11 epochs took 1.17 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.025187, T: 42000, Avg. loss: 0.485980\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.499699, T: 84000, Avg. loss: 0.364858\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.761864, T: 126000, Avg. loss: 0.334761\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.918558, T: 168000, Avg. loss: 0.324793\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.020727, T: 210000, Avg. loss: 0.321011\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.086728, T: 252000, Avg. loss: 0.319349\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.131595, T: 294000, Avg. loss: 0.318638\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.161525, T: 336000, Avg. loss: 0.318337\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.181575, T: 378000, Avg. loss: 0.318192\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.195608, T: 420000, Avg. loss: 0.318104\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.206860, T: 462000, Avg. loss: 0.318064\n",
      "Total training time: 1.24 seconds.\n",
      "Convergence after 11 epochs took 1.24 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.005086, T: 42000, Avg. loss: 0.490295\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.470062, T: 84000, Avg. loss: 0.374327\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.723210, T: 126000, Avg. loss: 0.346003\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.875392, T: 168000, Avg. loss: 0.336883\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.969827, T: 210000, Avg. loss: 0.333455\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.031824, T: 252000, Avg. loss: 0.332068\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.072604, T: 294000, Avg. loss: 0.331532\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.100453, T: 336000, Avg. loss: 0.331249\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.117822, T: 378000, Avg. loss: 0.331145\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.131392, T: 420000, Avg. loss: 0.331094\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.139341, T: 462000, Avg. loss: 0.331065\n",
      "Total training time: 2.22 seconds.\n",
      "Convergence after 11 epochs took 2.22 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.029422, T: 42000, Avg. loss: 0.488747\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.503548, T: 84000, Avg. loss: 0.366420\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.765427, T: 126000, Avg. loss: 0.335955\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.923419, T: 168000, Avg. loss: 0.325892\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.024646, T: 210000, Avg. loss: 0.321963\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.091061, T: 252000, Avg. loss: 0.320310\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.135561, T: 294000, Avg. loss: 0.319574\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.164661, T: 336000, Avg. loss: 0.319220\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.185121, T: 378000, Avg. loss: 0.319064\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.200265, T: 420000, Avg. loss: 0.319023\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.210069, T: 462000, Avg. loss: 0.318960\n",
      "Total training time: 2.52 seconds.\n",
      "Convergence after 11 epochs took 2.52 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.015175, T: 42000, Avg. loss: 0.489384\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.486042, T: 84000, Avg. loss: 0.370389\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 708, Bias: -1.743912, T: 126000, Avg. loss: 0.340967\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.898262, T: 168000, Avg. loss: 0.331398\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 708, Bias: -1.995403, T: 210000, Avg. loss: 0.327776\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.059386, T: 252000, Avg. loss: 0.326258\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.102516, T: 294000, Avg. loss: 0.325605\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.131810, T: 336000, Avg. loss: 0.325311\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.151716, T: 378000, Avg. loss: 0.325150\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 708, Bias: -2.165829, T: 420000, Avg. loss: 0.325109\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lambdaa = [0,10,100]\n",
    "lambda_er = []\n",
    "\n",
    "for ii in lambdaa:\n",
    "    \n",
    "    clf = linear_model.SGDClassifier(eta0=0.0001, alpha=ii, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "    clf.fit(X=mnist_train, y=train_labels)\n",
    "    label_pred = clf.predict(mnist_dev)\n",
    "    lambda_er.append(1-accuracy_score(dev_labels, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambda_er)\n",
    "lambdaa[np.where(lambda_er==np.min(lambda_er))[0][0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse160",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
